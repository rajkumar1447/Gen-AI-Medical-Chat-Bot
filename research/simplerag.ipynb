{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec98a5b",
   "metadata": {},
   "source": [
    "# STEPS FOR THE DATA STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Data Ingestion.\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Natural_language_processing\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Chunking.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Embeddings.\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cpu\"},  # or \"cuda\" if you have GPU\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "embeddings = embedding_model.embed_documents([chunk.page_content for chunk in chunks])\n",
    "print(f\"Embedded {len(embeddings)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Initialize Qdrant VectorStore\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "qdrant = QdrantClient(path=\"./qdrant_data\")  # Local, or use `host` and `port` for remote\n",
    "\n",
    "# Create collection (if not exists)\n",
    "collection_name = \"rag_demo\"\n",
    "if collection_name not in qdrant.get_collections().collections:\n",
    "    qdrant.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b27143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Store vectors in Qdrant\n",
    "db = Qdrant.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    client=qdrant,\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LLM.\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    temperature=0.2,\n",
    "    max_new_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- RETRIEVER ----\n",
    "retriever = qdrant_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b326fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Design ChatPrompt Template.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt= ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an intelligent assistant helping users based on the following retrieved context.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "{question}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0380ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain Instruction\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Create Retriever from Qdrant vector store\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",  # or \"mmr\" for Max Marginal Relevance\n",
    "    search_kwargs={\"k\": 5}      # top 5 similar documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ad6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f12f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
